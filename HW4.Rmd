---
title: "HW4"
author: "Jiarui Chang"
date: "2020/10/6"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1
```{r, message = F, warning= F}
library(MASS)
library(car)
library(glmnet)
```

```{r}
Data <- birthwt
head(birthwt)
```

```{r}
fit <- lm(bwt ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = Data)
summary(fit)
```

```{r}
vif(fit)
```
\
From the result above we can find out that vif value for all parameters are much far away from 10, thus we can say that there is no multicollinearity.
\

## b
```{r}
set.seed(1111)
n <- dim(Data)[1]
ind <- sample(n, 0.1*n, replace = F)
data_test <- Data[ind,]
data_train <- Data[-ind,]
fit2 <- lm(bwt ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = data_train)
mse_train <- mean(fit2$residuals^2)
summary(fit2)
test_x <- cbind(1,as.matrix(data_test[,2:9]))
test_y <- as.matrix(data_test[,10])
beta_ols <- as.matrix(fit2$coefficients)
mse_test <- mean((test_y - test_x%*%beta_ols)^2)
mse_train
mse_test
```

```{r}
data_train <- as.matrix(data_train)
ridge.cv <- cv.glmnet(data_train[,2:9], data_train[,10],
                      lambda = seq(0,1000,length.out = 10000),
                      alpha = 0)
plot(ridge.cv)
lambda <- ridge.cv$lambda.min
ridge_result <- glmnet(data_train[,2:9], data_train[,10],
                       lambda = lambda, alpha = 0, intercept = T)
mse_train_ridge <- mean((data_train[,10]-data_train[,2:9]%*%ridge_result$beta-ridge_result$a0)^2)
mse_test_ridge <- mean((test_y-test_x[,2:9]%*%ridge_result$beta-ridge_result$a0)^2)
mse_train_ridge
mse_test_ridge
```
\
From the result we can find out that compare with ols ridge has a bigger train error but a lower test error. That may because ridge is doing a model selection, in that case some betas which is not significant are not include in ridge model.
\

## 2
```{r}
lasso.cv <- cv.glmnet(data_train[,2:9], data_train[,10],
                      lambda = seq(0,1000, length.out = 10000),
                      alpha = 1)
plot(lasso.cv)
lambda_lasso <- lasso.cv$lambda.min
lasso_result <- glmnet(data_train[,2:9], data_train[,10],
                       lambda = lambda_lasso, alpha = 1, intercept = T)
mse_train_lasso <- mean((data_train[,10]-data_train[,2:9]%*%lasso_result$beta-lasso_result$a0)^2)
mse_test_lasso <- mean((test_y-test_x[,2:9]%*%lasso_result$beta-lasso_result$a0)^2)
mse_train_lasso
mse_test_lasso
```

```{r}
aic_result <- stepAIC(fit)
```

```{r}
mse_aic_train <- mean(aic_result$residuals^2)
test_x_aic <- cbind(1,Data[,"lwt"], Data[,"ht"], Data[,"race"], Data[,"smoke"], Data[,"ui"])
mse_aic_test <- mean((Data[,"bwt"]-test_x_aic%*%aic_result$coefficients)^2)
mse_aic_train
mse_aic_test
```
\
From the result above we can find that compare with lasso regression, aic result have a lower train error but a much higher test error. 
\

```{r}
result <- matrix(c(3,2,2,2,
                   3,2,2,1,
                   1,3,3,3,
                   3,3,2,1,
                   1,1,2,3), nrow = 5, ncol = 4)
colnames(result) <- c("OLS", "Ridge", "Lasso", "Elastic Net")
rownames(result) <- c("Performance when p >> n",
                      "Performance under multicollinearity",
                      "Unbiased estimators",
                      "Model selection capability",
                      "Simplicity: Computation, Inference, Interpretation")
result
```


